{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxzSHcy23Yj3",
        "outputId": "b022b77c-8249-44ca-dc2e-1996e8da02a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Scraping: http://books.toscrape.com/catalogue/page-1.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-2.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-3.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-4.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-5.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-6.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-7.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-8.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-9.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-10.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-11.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-12.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-13.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-14.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-15.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-16.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-17.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-18.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-19.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-20.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-21.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-22.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-23.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-24.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-25.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-26.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-27.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-28.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-29.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-30.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-31.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-32.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-33.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-34.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-35.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-36.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-37.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-38.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-39.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-40.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-41.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-42.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-43.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-44.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-45.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-46.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-47.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-48.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-49.html\n",
            "Scraping: http://books.toscrape.com/catalogue/page-50.html\n",
            "\n",
            "Scraping complete! Data saved to books.csv.\n",
            "                                   Title    Price Availability Star Rating\n",
            "0                   A Light in the Attic  Â£51.77     In stock       Three\n",
            "1                     Tipping the Velvet  Â£53.74     In stock         One\n",
            "2                             Soumission  Â£50.10     In stock         One\n",
            "3                          Sharp Objects  Â£47.82     In stock        Four\n",
            "4  Sapiens: A Brief History of Humankind  Â£54.23     In stock        Five\n"
          ]
        }
      ],
      "source": [
        "#q1\n",
        "!pip install requests beautifulsoup4 pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_books():\n",
        "    base_url = \"http://books.toscrape.com/catalogue/\"\n",
        "    current_page_url = base_url + \"page-1.html\"\n",
        "    all_books_data = []\n",
        "\n",
        "    while current_page_url:\n",
        "        print(f\"Scraping: {current_page_url}\")\n",
        "        try:\n",
        "            response = requests.get(current_page_url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            books = soup.find_all(\"article\", class_=\"product_pod\")\n",
        "\n",
        "            for book in books:\n",
        "                title = book.h3.a[\"title\"]\n",
        "                price = book.find(\"p\", class_=\"price_color\").text\n",
        "                availability = book.find(\"p\", class_=\"instock availability\").text.strip()\n",
        "\n",
        "\n",
        "                rating = book.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
        "\n",
        "                all_books_data.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Price\": price,\n",
        "                    \"Availability\": availability,\n",
        "                    \"Star Rating\": rating\n",
        "                })\n",
        "\n",
        "            next_button = soup.find(\"li\", class_=\"next\")\n",
        "            if next_button:\n",
        "                next_page_relative_url = next_button.a[\"href\"]\n",
        "                current_page_url = base_url + next_page_relative_url\n",
        "            else:\n",
        "                current_page_url = None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_books_data\n",
        "\n",
        "books_data = scrape_books()\n",
        "\n",
        "if books_data:\n",
        "    df = pd.DataFrame(books_data)\n",
        "    df.to_csv(\"books.csv\", index=False)\n",
        "    print(\"\\nScraping complete! Data saved to books.csv.\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"No data was scraped.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#q2\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "import pandas as pd\n",
        "\n",
        "def setup_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "    return webdriver.Chrome(options=options)\n",
        "\n",
        "def scrape_imdb_robust():\n",
        "    url = \"https://www.imdb.com/chart/top/\"\n",
        "    driver = setup_driver()\n",
        "    all_movies_data = []\n",
        "\n",
        "    try:\n",
        "        print(\"Fetching IMDB Top 250 page...\")\n",
        "        driver.get(url)\n",
        "\n",
        "\n",
        "        wait = WebDriverWait(driver, 20)\n",
        "        movie_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.ipc-metadata-list-summary-item__c\")))\n",
        "\n",
        "        print(f\"Successfully located {len(movie_elements)} movie containers. Starting scrape...\")\n",
        "\n",
        "        for element in movie_elements:\n",
        "            try:\n",
        "                title_text = element.find_element(By.CSS_SELECTOR, \"h3.ipc-title__text\").text\n",
        "                rank_str, title = title_text.split('. ', 1)\n",
        "\n",
        "                metadata_items = element.find_elements(By.CSS_SELECTOR, \"span.cli-title-metadata-item\")\n",
        "                year = metadata_items[0].text if metadata_items else \"N/A\"\n",
        "\n",
        "                rating_span = element.find_element(By.CSS_SELECTOR, \"span.ipc-rating-star\")\n",
        "                rating = rating_span.text.split()[0]\n",
        "\n",
        "                all_movies_data.append({\n",
        "                    \"Rank\": int(rank_str),\n",
        "                    \"Movie Title\": title,\n",
        "                    \"Year of Release\": year,\n",
        "                    \"IMDB Rating\": float(rating)\n",
        "                })\n",
        "            except (NoSuchElementException, IndexError, ValueError):\n",
        "                continue\n",
        "\n",
        "    except TimeoutException:\n",
        "        print(\"Scraping failed: The movie list did not load within the 20-second time limit.\")\n",
        "        print(\"The website structure has likely changed, or anti-scraping measures are blocking the script.\")\n",
        "    except Exception as e:\n",
        "        print(f\"A critical error occurred: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return all_movies_data\n",
        "\n",
        "imdb_data = scrape_imdb_robust()\n",
        "\n",
        "if imdb_data:\n",
        "    df = pd.DataFrame(imdb_data)\n",
        "    df.to_csv(\"imdb_top250.csv\", index=False)\n",
        "    print(f\"\\nScraping complete! Found {len(imdb_data)} movies. Data saved to imdb_top250.csv.\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"\\nNo data was scraped from IMDB.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LGO3UnS599F",
        "outputId": "f7449d45-1389-48c2-ed6e-ee54532409b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (4.35.0)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,917 B in 1s (2,870 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[1;31mE: \u001b[0mdpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. \u001b[0m\n",
            "Fetching IMDB Top 250 page...\n",
            "Successfully located 250 movie containers. Starting scrape...\n",
            "\n",
            "Scraping complete! Found 250 movies. Data saved to imdb_top250.csv.\n",
            "   Rank               Movie Title Year of Release  IMDB Rating\n",
            "0     1  The Shawshank Redemption            1994          9.3\n",
            "1     2             The Godfather            1972          9.2\n",
            "2     3           The Dark Knight            2008          9.1\n",
            "3     4     The Godfather Part II            1974          9.0\n",
            "4     5              12 Angry Men            1957          9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#q3\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_weather():\n",
        "    url = \"https://www.timeanddate.com/weather/\"\n",
        "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    response.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    all_weather_data = []\n",
        "\n",
        "\n",
        "    table = soup.find(\"table\", class_=\"zebra fw tb-theme\")\n",
        "    rows = table.find_all(\"tr\")\n",
        "\n",
        "    for row in rows[1:]:\n",
        "        cols = row.find_all(\"td\")\n",
        "        if len(cols) >= 3:\n",
        "            city = cols[0].text.strip()\n",
        "            temperature = cols[1].text.strip()\n",
        "            condition = cols[2].text.strip()\n",
        "\n",
        "            all_weather_data.append({\n",
        "                \"City\": city,\n",
        "                \"Temperature\": temperature,\n",
        "                \"Condition\": condition\n",
        "            })\n",
        "\n",
        "    return all_weather_data\n",
        "\n",
        "\n",
        "\n",
        "weather_data = scrape_weather()\n",
        "\n",
        "if weather_data:\n",
        "    df = pd.DataFrame(weather_data)\n",
        "    df.to_csv(\"weather.csv\", index=False)\n",
        "    print(\"✅ Scraping complete! Data saved to weather.csv\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"⚠️ No data was scraped from Time and Date.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKDMB4guBNM-",
        "outputId": "b0fe996d-259b-4fd0-d6fa-aa14ae6692f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Scraping complete! Data saved to weather.csv\n",
            "          City  Temperature Condition\n",
            "0        Accra  Wed 3:45 pm          \n",
            "1  Addis Ababa  Wed 6:45 pm          \n",
            "2     Adelaide  Thu 1:15 am          \n",
            "3      Algiers  Wed 4:45 pm          \n",
            "4       Almaty  Wed 8:45 pm          \n"
          ]
        }
      ]
    }
  ]
}